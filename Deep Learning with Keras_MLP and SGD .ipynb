{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Deep learning has recently significantly advanced the state of the art in machine learning. It seems almost like the field has found the magic bullet that can do it all, from face recognition to natural language processing. The goal of this notebook and the following notebooks is to first explain the concepts behind deep learning on an intuitive level with practical tips, enabeling you to get up and running with designing your own deep networks and training them with keras and tensorflow. Second, I am trying to convey a sense of what makes deep learning useful, how to gain insights into what the networks have learned, and last not least what the limitations are and in what cases other classification or regression methods might be a better tool for the job. \n",
    "\n",
    "# Logistics\n",
    "This notebook assumes that you have [keras](https://keras.io/) installed on your system. Keras is a high level deep learning library, which recently became the official high level front end for [tensorflow](https://www.tensorflow.org/). For the purpose of this notebook it is also fine to have the [theano](http://deeplearning.net/software/theano/install.html) backend for keras instead of tensorflow. Under windows I had more success with getting theano to run with GPU support than tensorflow, but these setups might change. Also this notebook uses Python 3.5, because it is the python version supported by tensorflow under windows. If you are under Linux you should have non problem to get either of these libraries to run with python 3.5. \n",
    "\n",
    "To make sure that you don't get frustrated by having to install additional packages later on in the notebook, I list all imporant imports here at the beginning in the order they are needed later on. So if you can execute the following cell without any import errors you are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports necessary for this notebook\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning a brief history\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The perceptron\n",
    "\n",
    "The perceptron is the basic unit of a deep neural network. As you read above, it caused quite a hype when it was invented, because it can be interpreted as a very simplified model of a neuron. However, we are also going to see that it actually is nothing else than a separating hyperplane, that can be fitted between two point clouds for classification. \n",
    "\n",
    "Let's start with the concept of the separating hyperplane. \n",
    "\n",
    "<img src=\"./Figures/DL_SeparatingHyperplane.png\" width=\"25%\" height=\"25%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows two point clouds. Each point stands for a sample in our training set. If we find a good position for a separating hyperplane (the green line) between the two point clouds , we can make predictions for other points, that are not part of our training set. If a point lies on the right side of the hyperplane we label it as yellow, if it lies on the left side we label it as blue. We can describe the separating hyperplane with an orthogonal vector $w$ and an offset vector $b$. The vector $w$ describes the orientation of the hyperplane. If we rotate $w$, then the hyperplane rotates as well, because it defined as always being orthogonal to $w$. The vector is called $w$ because we will later see that it describes the weights of the perceptron. The offset vector is used to shift our hyperplane around. The b stands for bias. Training this classifier consists of finding values for $w$ and $b$, that separate the two point clouds. But before we discuss the training, let's think a bit more about how the classification works, once we know the parameters $w$ and $b$. \n",
    "\n",
    "For classification of a new data point $x$ we want to determine its estimated label $\\hat{y}$. As we can see from the figure above, this means we need to determine if the new point that we want to classifier is on the right or the left side of the hyperplane. All we have to do is evaluate the expression $w^T x + b$ for the new point $x$. If $w^T x + b < 0$ then the point $x$ is on the left side of the hyperplane, so we label it blue. If $w^T x + b > 0$ then it is on the right side of the hyperplane and we label it yellow. For the points directly on the hyperplane we have $w^T x + b = 0$. For these points we need to make an arbitrary decision if we want to label them blue or yellow, because the hyperplane doesn't give us enough information. We could resolve these points by assigning a fixed label, like always label it blue. For a medical application for example, we might want to err on the side of caution and recommend treatment. \n",
    "\n",
    "What we have discussed so far, is a very simple linear classifier, but it also can be interpreted as a simplified model of a neuron! Consider the condition for the points on the right side of the hyperplane $w^T x + b > 0$ for a three dimensional point space: Then $x = [x_1, x_2, x_3]^T$, $w = [w_1, w_2, w_3]$, and $b = [b_1]$. So our condition becomes: $w_1 x_1 + w_2 x_2 + w_3 x_3 + b > 0$. We can draw this in a diagram as follows:\n",
    "\n",
    "<img src=\"./Figures/DL_Perceptron.png\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this diagram, each of the input dimensions of $x$ is considered an input to our \"neuron\". The neuron then multiplies each input with the specific weight from the weight vector $w$. The bias becomes just an extra weight, with the constant input 1. If you compare this to the drawing of a real neuron, we have a dendritic tree, collecting information. The sum sign then corresponds to the cell body, where all the input signal is aggregated. The only aspect missing is to determine if the neuron should \"fire\"or not, aka the estimated label for $\\hat{y}$. To model this last step, we could use a step function, shown in the following figure on the left side:\n",
    "\n",
    "<img src=\"./Figures/DL_StepVsSigmoid.png\" width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "As you can see the step function returns zero as long as the sum over the inputs is smaller than zero, so all points to the left of our hyperplane would be mapped to the label zero if we apply this step function to our hyperplane equation $w^T x + b$. If the sum becomes greater than zero, the step function returns one. The problem with the step function though is that it is not continuous and thus doesn't have a derivative. Ultimately, when we train our classifier, we need to compute a gradient to find values for our parameters $w$ and $b$. So instead of using the step function on the left, we can use the sigmoid function $s(z)$ shown on the right side in the figure above. The sigmoid looks approximately like the step function, but is continuous. This function is defined as $s(z) = \\frac{1}{1+\\exp{-z}}$. You might already know this function because it is also used for logistic regression. If we put all the pieces together now, we have the perceptron computing $\\hat{y} = s(w^Tx+b)$. Which actually is the equation for logistic regression and indeed both were invented only two years apart from each other [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) was developed by David Cox in 1958, and the [perceptron](https://en.wikipedia.org/wiki/Perceptron) was developed by Frank Rosenblatt in 1957. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron example\n",
    "\n",
    "While a single perceptron is not a deep classifier, let's start with getting to know keras a bit and motivate the next theory part about optimization. In this example we will use a perceptron to classify image patches. We will generate two types of 3x3 image patches. One type with a horizontal stripe in the middle, and one with a vertical stripe in the middle. Then we use a perceptron to learn which class a patch belongs to. This setup is very simple and that means we have complete control over it and can learn how the network behaves if we add noise to the patches, etc. So let's get started!\n",
    "\n",
    "We need to program the following parts:\n",
    "\n",
    "- a function to generate an image patch for the given class label\n",
    "- a function to generate our training data\n",
    "- keras parts to train the classifier\n",
    "- function to visualize the learned filter\n",
    "\n",
    "Write a function to generate a 3x3 image patch for a given class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "generate_middle_stripe_patch\n",
    "\n",
    "Given a label, return a 3x3 image patch \n",
    "with a line in the middle of the patch. \n",
    "The line should be horizontal for label 0 \n",
    "and vertical for label 1. Background should\n",
    "be encoded as 0 and the line as 1. \n",
    "\n",
    "Inputs\n",
    "------\n",
    "label : int\n",
    "    The label of the image patch to be generated\n",
    "\n",
    "Returns\n",
    "-------\n",
    "patch : 3x3 numpy array\n",
    "    A patch with a stripe in the middle \n",
    "    according to the given label\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> generate_middle_stripe_patch(0)\n",
    "array([[ 0.,  0.,  0.],\n",
    "       [ 1.,  1.,  1.],\n",
    "       [ 0.,  0.,  0.]])\n",
    ">>> generate_middle_stripe_patch(1)\n",
    "array([[ 0.,  1.,  0.],\n",
    "       [ 0.,  1.,  0.],\n",
    "       [ 0.,  1.,  0.]])\n",
    "\n",
    "\"\"\"    \n",
    "#your code here\n",
    "\n",
    "def generate_middle_stripe_patch(label):\n",
    "    patch = np.zeros((3,3))\n",
    "    if label==0:\n",
    "        patch[1,:] = 1.\n",
    "    elif label==1:\n",
    "        patch[:,1] = 1.\n",
    "    else:\n",
    "        print(\"This function is only defined for label==0 or label==1\")\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to generate our training data. In a real world scenario our image patches would contain noise, and would be much harder to classify. To already get familiar with the standard setup for these scenarios, let's generate 100 training samples, although there clearly would be a lot of duplicates and only two really different patches in our training set at this point. \n",
    "\n",
    "First write the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "generate_middle_stripe_data\n",
    "\n",
    "Given the number of samples, generate a data matrix\n",
    "number of samples x number of dimensions\n",
    "and the corresponding label vector\n",
    "\n",
    "Inputs\n",
    "------\n",
    "n_samples : int\n",
    "    Number of samples for the data matrix\n",
    "\n",
    "Returns\n",
    "-------\n",
    "data : n_samples x 9 numpy array\n",
    "    A matrix with n_samples rows and each\n",
    "    row containing the 3x3=9 pixels of an image\n",
    "    patch\n",
    "labels : vector of length n_samples\n",
    "    An array with the class labels corresponding\n",
    "    to the points in the data matrix\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> generate_middle_stripe_data(2)\n",
    "(array([[ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.]]), \n",
    " array([0, 1]))\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "def generate_middle_stripe_data(n_samples):\n",
    "    # generate the labels\n",
    "    labels = np.random.randint(0,2,n_samples)\n",
    "    \n",
    "    # generate corresponding data\n",
    "    data = np.zeros((n_samples,9))\n",
    "    for i,l in enumerate(labels):\n",
    "        patch = generate_middle_stripe_patch(l)\n",
    "        data[i,:] = patch.flatten()\n",
    "        \n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a training set with 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, labels_train = generate_middle_stripe_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the preparations in place to get started with using keras! Keras has two interfaces: a squential model and a functional API. We will start with the sequential model first, as it is the easier one of the two. If you get more advanced you will need the functional API, but for now the sequential model is really all we need. \n",
    "\n",
    "The following lines of code create an empty model first and then add a dense layer with 9 input units and just one ouput unit with a sigmoid activation. We need 9 input units, because our data comes from a 3x3 image patch, so we have 9 pixels in each patch, which means each data point of our training data has 9 dimensions. Compare this with our perceptron figure above. Our $x$ is now 9 dimensional, we still have one bias $b$. The weight vector $w$ has as many dimensions as $x$, so it also has 9 dimensions. We have one \"cell body\" where all the iputs get summed up. This is the one output unit we have, and then we have the sigmoid activation as a continous approximation of the step function to decide which label we assign to the input data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty model\n",
    "model = Sequential()\n",
    "# add one densely connected unit\n",
    "model.add(Dense(1, input_dim=9, kernel_initializer=Constant(value=0)))\n",
    "# add the activation function\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# print a model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary above you can see that our model has 10 parameters. These are the 9 weights from the vector $w$ plus the one bias $b$. The sigmoid function doesn't have any parameters, so the number is 0 for this part. \n",
    "\n",
    "So we have our model and our training data. All that is left to do is to actually train the model. Training means that we need to find good values for $w$ and $b$ such that our classifier can distinguish the two different image patches. Keras has a number of pre-defined optimizers and loss functions that can be used for this purpose. For now we will just use binary_crossentropy as our loss function and stochastic gradient decent (SGD) as the optimizer to get some results and see some output. Further below you can read about what this loss function and optimizer mean and how they work. \n",
    "\n",
    "Training the model consists of two parts in keras. First we need to configure the training process by compiling our model together with the optimizer and the loss function we want to use. After this step comes the actual training with the _fit()_ function. You can see that this function gets the data and the labels as parameters. In addition there is a number of epochs and a batch size defined. The number of epochs specifies how many times the classifier is trained on the whole set of training data. Each iteration over the whole data is split up into smaller batches. So with 100 training samples and a batch size of 10 the network looks at the first 10 samples, trains, looks at 10 more, trains, etc. After 10 steps it has seen all 100 samples and one epoch is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s - loss: 0.6822 - acc: 0.9500     \n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s - loss: 0.6581 - acc: 1.0000     \n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s - loss: 0.6354 - acc: 1.0000     \n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s - loss: 0.6136 - acc: 1.0000     \n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s - loss: 0.5929 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f1e8afde48>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data_train, labels_train, epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer at the output from the training. Each iteration outputs something like this:\n",
    "\n",
    "`Epoch 1/15\n",
    "100/100 [==============================] - 0s - loss: 0.9685 - acc: 0.4700  `   \n",
    "\n",
    "The first line shows the current epoch and the maximum number of epochs defined in the _fit()_ function. The second line shows how many samples of the training data have already been looked at in the current epoch and then we get a nice little progress bar and a time estimate how long the current epoch will need to finish. For larger networks this time estimate is especially useful. If you know your network needs 500 iterations to train and each epoch takes a minute, you might want to let it run over night isntead of slowing your computer down during the day. The two remaining values are the current loss and the accuracy on the training data. The loss should go down with training, and the accuracy should go over over time. In our example you can see that the accuracy actually became perfect in the last iterations so our classifier should not do any mistakes now. Let's see if that is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for patch with label 0: \n",
      "1/1 [==============================] - 0s\n",
      "[[0]]\n",
      "\n",
      "Output for patch with label 1: \n",
      "1/1 [==============================] - 0s\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# generate a patch for label 0\n",
    "patch = generate_middle_stripe_patch(0)\n",
    "patch = patch.reshape(1,9)\n",
    "\n",
    "#get the prediction from the network\n",
    "print(\"Output for patch with label 0: \")\n",
    "print(model.predict_classes(patch))\n",
    "print()\n",
    "\n",
    "# generate a patch for label 1\n",
    "patch = generate_middle_stripe_patch(1)\n",
    "patch = patch.reshape(1,9)\n",
    "\n",
    "#get the prediction from the network\n",
    "print(\"Output for patch with label 1: \")\n",
    "print(model.predict_classes(patch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good! Let's see what the learned weights $w$ look like. Remember, the dimensionality of $w$ is the same as the dimensionality of $x$. Our $x$ is a 3x3 image patch, so we can also reshape $w$ as a 3x3 image patch and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4SGpieEhqcn/A4++BWYxyz35AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f1e7c12518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the weights from our model\n",
    "w,b = model.layers[0].get_weights()\n",
    "w = w.reshape((3,3))\n",
    "\n",
    "plt.imshow(w, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
